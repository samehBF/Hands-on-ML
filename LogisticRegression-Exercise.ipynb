{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Click-Through Rate Prediction**\n",
    "#### This exercise covers the steps for creating a click-through rate (CTR) prediction pipeline. \n",
    "#### ** This exercise will cover: **\n",
    "+  ####*Part 1:* Parse CTR data and generate OHE features\n",
    "+  ####*Part 2:* Training with LogisticRegression and logloss evaluation\n",
    " \n",
    "#### Note that, for reference, you can look up the details of the relevant Spark methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1: Parse CTR data and generate OHE features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data loading and visualizing the raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('HandsOnML', 'dac_sample.txt')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "if os.path.isfile(fileName):\n",
    "    rawData = (sc\n",
    "               .textFile(fileName, 2)\n",
    "               .map(lambda x: x.replace('\\t', ',')))# work with either ',' or '\\t' separated data\n",
    "    print rawData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Splitting the data **\n",
    "####  1- Splitting the data into training, validation, and test sets using the [randomSplit method](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) with the specified weights and seed to create RDDs storing each of these datasets.\n",
    "#### 2 - [Cache](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cache) each of these RDDs. \n",
    "#### 3- Compute the size of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "weights = [.8, .1, .1]\n",
    "seed = None\n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainData, rawValidationData, rawTestData = <FILL IN>\n",
    "# Cache the data because we need to use these datasets later\n",
    "<FILL IN>\n",
    "<FILL IN>\n",
    "<FILL IN>\n",
    "\n",
    "nTrain = <FILL IN>\n",
    "nVal = <FILL IN>\n",
    "nTest = <FILL IN>\n",
    "print nTrain, nVal, nTest, nTrain + nVal + nTest\n",
    "print rawData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Creating ONE Dictionary**\n",
    "#### Generating a dictionary containing a list of \"distinct (featureID, value) : Unique Integer\" from the raw training data. We will ignore the first field (which is the 0-1 label), and parse the remaining fields (or raw features). Note : In our exercise, on apply the OneHot Encoding on all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fist we should get a RDD of a list of (featureID,value) from the raw training data\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def parseData(data):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        data (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_chain = <FILL IN>\n",
    "    \n",
    "    fv_tuple = <FILL IN>\n",
    "    \n",
    "    return fv_tuple\n",
    "    \n",
    "parsedTrainFeat = rawTrainData.map(parseData)\n",
    "print parsedTrainFeat.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an OHE dictionary from the RDD of the list of (featureID, value)\n",
    "# generated in the previous function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### OHE dictionary sample : \n",
    "#cityOHEDict[(0,'Beijing')] = 0\n",
    "#cityOHEDict[(0,'Paris')] = 1\n",
    "#cityOHEDict[(0,'London')] = 2\n",
    "#cityOHEDict[(0,'New York')] = 3\n",
    "#cityOHEDict[(1, 'Asia')] = 4\n",
    "#cityOHEDict[(1, 'Europe')] = 5\n",
    "#cityOHEDict[(1, 'American')] = 6\n",
    "#cityOHEDict[(2, 'very much')] = 7\n",
    "#cityOHEDict[(2, 'a little')] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def createOneHotDictionary(inputData):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "    \n",
    "    Hint : function zipWithIndex can be useful\n",
    "\n",
    "    Args:\n",
    "        inputData (RDD of lists of (int, str)): An RDD of observations where each observation is\n",
    "            made up of a list of (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dictionary: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "    \"\"\"\n",
    "    \n",
    "    DistinctFeatures = <FILL IN>\n",
    "    \n",
    "    OHEDictionary = <FILL IN>\n",
    "    \n",
    "    return OHEDictionary\n",
    "\n",
    "OHEDictionary = createOneHotDictionary(parsedTrainFeat)\n",
    "print OHEDictionary.items()[0]\n",
    "\n",
    "numCtrOHEFeats = len(OHEDictionary.keys())\n",
    "print numCtrOHEFeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Extracting the features using OHE dictionary and Transforming training/validation/test data to a RDD of LabeledPoint **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def oneHotEncoding(parsedFeats, OHEDictionary):\n",
    "    \"\"\"Produce a OneHot encoding from \n",
    "         1- a list of features parsed from raw data\n",
    "         2- an OneHot dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted.\n",
    "        !Important!:Because some categorical values will likely appear in validation/Test data \n",
    "        that did not exist in the training data. \n",
    "        To deal with this situation, ignoring unseen categories in validation/test data\n",
    "\n",
    "    Args:\n",
    "        parsedFeats (list of (int, str)): The features corresponding to a single observation.Each\n",
    "            feature consists of a tuple of featureID and the feature's value.\n",
    "        OHEDictionary: A mapping of (featureID, value) to unique integer.\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length numOHEFeats with indicies equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    return <FILL IN>\n",
    "\n",
    "def getLabeledPoint(data, OHEDictionary):\n",
    "    \"\"\"Obtain the label and feature vector for this raw observation.\n",
    "\n",
    "    Note:\n",
    "        You must use the function `oneHotEncoding` in this implementation.\n",
    "    Args:\n",
    "        data (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "        OHEDictionary (dictionary of (int, str) to int): Mapping of (featureID, value) to unique integer.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: Contains the label for the observation and the one-hot-encoding of the\n",
    "            raw features based on the provided OHE dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    parsedFeat = <FILL IN>\n",
    "    \n",
    "    sparseVector = <FILL IN>\n",
    "    \n",
    "    return <FILL IN>\n",
    "\n",
    "OHETrainData = rawTrainData.map(lambda data: getLabeledPoint(data, OHEDictionary))\n",
    "OHETrainData.cache()\n",
    "print OHETrainData.take(1)\n",
    "\n",
    "OHEValidationData = rawValidationData.map(lambda data: getLabeledPoint(data, OHEDictionary))\n",
    "OHEValidationData.cache()\n",
    "print OHEValidationData.take(1)\n",
    "\n",
    "OHETestData = rawValidationData.map(lambda data: getLabeledPoint(data, OHEDictionary))\n",
    "OHETestData.cache()\n",
    "print OHETestData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2: CTR prediction and logloss evaluation **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Log loss **\n",
    "#### Throughout this exercise, we will use log loss to evaluate the quality of models.  Log loss is defined as: $$  \\begin{align} \\scriptsize \\ell_{log}(p, y) = \\begin{cases} -\\log (p) & \\text{if } y = 1 \\\\\\ -\\log(1-p) & \\text{if } y = 0 \\end{cases} \\end{align} $$ where $ \\scriptsize p$ is a probability between 0 and 1 and $ \\scriptsize y$ is a label of either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from math import log\n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    \n",
    "    if y == 1.:\n",
    "        return <FILL IN>\n",
    "    if y == 0.:\n",
    "        return <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Predicted probability **\n",
    "#### In order to compute the log loss for the model we train, we need to write code to generate predictions from the model. Write a function that computes the raw linear prediction (t = x.dot(w) + intercept) from this logistic regression model and then passes it through a [sigmoid function](http://en.wikipedia.org/wiki/Sigmoid_function) $ \\scriptsize \\sigma(t) = (1+ e^{-t})^{-1} $ to return the model's probabilistic prediction. Then compute probabilistic predictions on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from math import exp #  exp(-t) = e^-t\n",
    "\n",
    "def getProbability(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's bias.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = <FILL IN>\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    \n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    \n",
    "    return <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Evaluate the model **\n",
    "#### Now we write a general function that takes as input a model and data, and outputs the log loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def computeLogLossOfModel(model, data):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Args:\n",
    "        model (LogisticRegressionModel): A trained logistic regression model.\n",
    "        data (RDD of LabeledPoint): Labels and features for each observation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    probability_label_tuple = <FILL IN>\n",
    "    \n",
    "    loss = <FILL IN>\n",
    "    \n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Traning the model with Logistic regression **\n",
    "####  First use [LogisticRegressionWithSGD](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithSGD) to train serveral models using `OHETrainData` with the given differente hyperparameter configuration. Getting the best model using validation dataset by calculating Log Loss with the parameters of every model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "numIters = 80\n",
    "regType = 'l2'\n",
    "includeIntercept = True\n",
    "\n",
    "# Initialize variables\n",
    "bestModel = None\n",
    "bestLogLoss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "stepSizes = [1,10]\n",
    "regParams = [1e-6,1e-3]\n",
    "for stepSize in stepSizes:\n",
    "    for regParam in regParams:\n",
    "        model = <FILL IN>\n",
    "        #calculate the log loss on validation dataset using the trained model\n",
    "        logLossValidation = <FILL IN>\n",
    "        print ('\\tstepSize = {0:.1f}, regParam = {1:.0e}: logloss = {2:.3f}'\n",
    "               .format(stepSize, regParam, logLossVa))\n",
    "        if (logLossValidation < bestLogLoss):\n",
    "            bestModel = model\n",
    "            bestLogLoss = logLossVa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Evaluate on the test set **\n",
    "#### Finally, do the predictions with the best model on the test set and calcule the accuracy of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pyspark.mllib.evaluation import BinaryClassificationMetrics from spark 1.4\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# doing the prediction on the test dataset using the best model\n",
    "predictionAndLabel = <FILL IN>\n",
    "print predictionAndLabel.take(1)\n",
    "\n",
    "# calculate the accuracy of the predicton of the best model in this exercise\n",
    "accuracy = <FILL IN>\n",
    "print ('The accuracy on Test Dataset with the best model:\\n\\taccuracy = {0:.3f}'\n",
    "       .format(accuracy))\n",
    "\n",
    "#from spark 1.4\n",
    "#metrics = BinaryClassificationMetrics(predictionAndLabel) \n",
    "#print metrics.areaUnderROC()\n",
    "\n",
    "sortedWeights = sorted(bestModel.weights)\n",
    "print sortedWeights[:5], bestModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can go further using the hashing features for this exercise. Good Luck"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
