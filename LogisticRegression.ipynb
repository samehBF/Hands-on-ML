{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Click-Through Rate Prediction**\n",
    "#### This exercise covers the steps for creating a click-through rate (CTR) prediction pipeline. \n",
    "#### ** This exercise will cover: **\n",
    "+  ####*Part 1:* Parse CTR data and generate OHE features\n",
    "+  ####*Part 2:* CTR prediction and logloss evaluation\n",
    " + #### *Visualization 1:* ROC curve\n",
    " \n",
    "#### Note that, for reference, you can look up the details of the relevant Spark methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1: Parse CTR data and generate OHE features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('HandsOnML', 'dac_sample.txt')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "if os.path.isfile(fileName):\n",
    "    rawData = (sc\n",
    "               .textFile(fileName, 2)\n",
    "               .map(lambda x: x.replace('\\t', ',')))  # work with either ',' or '\\t' separated data\n",
    "    print rawData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Splitting the data **\n",
    "####  1- Splitting the data into training, validation, and test sets using the [randomSplit method](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) with the specified weights and seed to create RDDs storing each of these datasets.\n",
    "#### 2 - [Cache](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cache) each of these RDDs. \n",
    "#### 3- Compute the size of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainData, rawValidationData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "# Cache the data\n",
    "rawTrainData.cache()\n",
    "rawValidationData.cache()\n",
    "rawTestData.cache()\n",
    "\n",
    "nTrain = rawTrainData.count()\n",
    "nVal = rawValidationData.count()\n",
    "nTest = rawTestData.count()\n",
    "print nTrain, nVal, nTest, nTrain + nVal + nTest\n",
    "print rawData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Creating ONE Dictionary**\n",
    "#### Transforming the raw training data to an RDD containing a list of (featureID, value). We will ignore the first field (which is the 0-1 label), and parse the remaining fields (or raw features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def parseData(data):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        data (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_chain = data.split(\",\")\n",
    "    \n",
    "    fv_tuple = list(enumerate(data_chain[1:]))\n",
    "    \n",
    "    return fv_tuple\n",
    "    \n",
    "parsedTrainFeat = rawTrainData.map(parseData)\n",
    "print parsedTrainFeat.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an OHE dictionary from the RDD with the list of (featureID, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### OHE dictionary sample : \n",
    "#cityOHEDict[(0,'Beijing')] = 0\n",
    "#cityOHEDict[(0,'Paris')] = 1\n",
    "#cityOHEDict[(0,'London')] = 2\n",
    "#cityOHEDict[(0,'New York')] = 3\n",
    "#cityOHEDict[(1, 'Asia')] = 4\n",
    "#cityOHEDict[(1, 'Europe')] = 5\n",
    "#cityOHEDict[(1, 'American')] = 6\n",
    "#cityOHEDict[(2, 'very much')] = 7\n",
    "#cityOHEDict[(2, 'a little')] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def createOneHotDictionary(inputData):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        inputData (RDD of lists of (int, str)): An RDD of observations where each observation is\n",
    "            made up of a list of (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dictionary: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "    \"\"\"\n",
    "    \n",
    "    DistinctFeatures = (inputData\n",
    "                              .flatMap(lambda x: x).distinct())\n",
    "    \n",
    "    OHEDictionary = (DistinctFeatures\n",
    "                            .zipWithIndex().collectAsMap())\n",
    "    \n",
    "    return OHEDictionary\n",
    "OHEDictionary = createOneHotDictionary(parsedTrainFeat)\n",
    "numCtrOHEFeats = len(OHEDictionary.keys())\n",
    "print numCtrOHEFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We have 233286 features totally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Transform training data to RDD of LabeledPoint **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def oneHotEncoding(rawFeats, OHEDictionary):\n",
    "    \"\"\"Produce a 1-of-k encoding from a list of features and an 1-of-k dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted.\n",
    "        Because some categorical values will likely appear in validation/Test data \n",
    "        that did not exist in the training data. \n",
    "        To deal with this situation, ignoring unseen categories in validation/test data\n",
    "\n",
    "    Args:\n",
    "        rawFeats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value.\n",
    "        OHEDictionary: A mapping of (featureID, value) to unique integer.\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length numOHEFeats with indicies equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    return SparseVector(len(OHEDictionary),sorted([(OHEDictionary[x],1.0) for x in rawFeats if x in OHEDictionary]))\n",
    "def createOHELabeledPoint(data, OHEDictionary, numOHEFeats):\n",
    "    \"\"\"Obtain the label and feature vector for this raw observation.\n",
    "\n",
    "    Note:\n",
    "        You must use the function `oneHotEncoding` in this implementation.\n",
    "    Args:\n",
    "        data (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "        OHEDictionary (dictionary of (int, str) to int): Mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The number of unique features in the training dataset.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: Contains the label for the observation and the one-hot-encoding of the\n",
    "            raw features based on the provided OHE dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    parsedFeat = parseData(data)\n",
    "    \n",
    "    sparseVector = oneHotEncoding(parsedFeat, OHEDictionary)\n",
    "    \n",
    "    return LabeledPoint(data[0],sparseVector)\n",
    "\n",
    "OHETrainData = rawTrainData.map(lambda data: createOHELabeledPoint(data, OHEDictionary, numCtrOHEFeats))\n",
    "OHETrainData.cache()\n",
    "print OHETrainData.take(1)\n",
    "\n",
    "OHEValidationData = rawValidationData.map(lambda data: createOHELabeledPoint(data, OHEDictionary, numCtrOHEFeats))\n",
    "OHEValidationData.cache()\n",
    "print OHEValidationData.take(1)\n",
    "\n",
    "OHETestData = rawValidationData.map(lambda data: createOHELabeledPoint(data, OHEDictionary, numCtrOHEFeats))\n",
    "OHETestData.cache()\n",
    "print OHETestData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2: CTR prediction and logloss evaluation **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Log loss **\n",
    "#### Throughout this exercise, we will use log loss to evaluate the quality of models.  Log loss is defined as: $$  \\begin{align} \\scriptsize \\ell_{log}(p, y) = \\begin{cases} -\\log (p) & \\text{if } y = 1 \\\\\\ -\\log(1-p) & \\text{if } y = 0 \\end{cases} \\end{align} $$ where $ \\scriptsize p$ is a probability between 0 and 1 and $ \\scriptsize y$ is a label of either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from math import log\n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    \n",
    "    if y == 1.:\n",
    "        return -log(p+epsilon)\n",
    "    if y == 0.:\n",
    "        return -log(1-p+epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Predicted probability **\n",
    "#### In order to compute the log loss for the model we train, we need to write code to generate predictions from the model. Write a function that computes the raw linear prediction from this logistic regression model and then passes it through a [sigmoid function](http://en.wikipedia.org/wiki/Sigmoid_function) $ \\scriptsize \\sigma(t) = (1+ e^{-t})^{-1} $ to return the model's probabilistic prediction. Then compute probabilistic predictions on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from math import exp #  exp(-t) = e^-t\n",
    "\n",
    "def getProbability(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w) + intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    \n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    \n",
    "    return (1. + exp(-rawPrediction))**(-1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Evaluate the model **\n",
    "#### To do this, write a general function that takes as input a model and data, and outputs the log loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def evaluateResults(model, data):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Args:\n",
    "        model (LogisticRegressionModel): A trained logistic regression model.\n",
    "        data (RDD of LabeledPoint): Labels and features for each observation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    prob_lb_tup = data.map(lambda lp: (getProbability(lp.features, model.weights, model.intercept),lp.label))\n",
    "    \n",
    "    loss_tup = prob_lb_tup.map(lambda (x,y): computeLogLoss(x,y))\n",
    "    \n",
    "    return loss_tup.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Traning the model with Logistic regression **\n",
    "####  First use [LogisticRegressionWithSGD](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithSGD) to train a model using `OHETrainData` with the given hyperparameter configuration. Next, use the `LogisticRegressionModel.weights` and `LogisticRegressionModel.intercept` attributes to print out the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "numIters = 80\n",
    "regType = 'l2'\n",
    "includeIntercept = True\n",
    "\n",
    "# Initialize va\"riables using values from initial model training\n",
    "bestModel = None\n",
    "bestLogLoss = 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "stepSizes = [1,10]\n",
    "regParams = [1e-6,1e-3]\n",
    "for stepSize in stepSizes:\n",
    "    for regParam in regParams:\n",
    "        model = (LogisticRegressionWithSGD\n",
    "                 .train(OHETrainData, numIters, stepSize, regParam=regParam, regType=regType,\n",
    "                        intercept=includeIntercept))\n",
    "        logLossVa = evaluateResults(model, OHEValidationData)\n",
    "        print ('\\tstepSize = {0:.1f}, regParam = {1:.0e}: logloss = {2:.3f}'\n",
    "               .format(stepSize, regParam, logLossVa))\n",
    "        if (logLossVa < bestLogLoss):\n",
    "            bestModel = model\n",
    "            bestLogLoss = logLossVa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualization: ROC curve **\n",
    "#### Graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "labelsAndScores = OHEValidationData.map(lambda lp:\n",
    "                                            (lp.label, getProbability(lp.features, bestModel.weights, bestModel.intercept)))\n",
    "labelsAndWeights = labelsAndScores.collect()\n",
    "labelsAndWeights.sort(key=lambda (k, v): v, reverse=True)\n",
    "labelsByWeight = np.array([k for (k, v) in labelsAndWeights])\n",
    "\n",
    "length = labelsByWeight.size\n",
    "truePositives = labelsByWeight.cumsum()\n",
    "numPositive = truePositives[-1]\n",
    "falsePositives = np.arange(1.0, length + 1, 1.) - truePositives\n",
    "\n",
    "truePositiveRate = truePositives / numPositive\n",
    "falsePositiveRate = falsePositives / (length - numPositive)\n",
    "\n",
    "def preparePlot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n",
    "                gridWidth=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hideLabels: axis.set_ticklabels([])\n",
    "    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax\n",
    "\n",
    "# Generate layout and plot data\n",
    "fig, ax = preparePlot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1))\n",
    "ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.plot(falsePositiveRate, truePositiveRate, color='#8cbfd0', linestyle='-', linewidth=3.)\n",
    "plt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)  # Baseline model\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3e) Evaluate on the test set **\n",
    "#### Finally, evaluate the best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Log loss for the best model from (5d)\n",
    "logLossTest = evaluateResults(bestModel, OHETestData)\n",
    "\n",
    "print ('Test Dataset Log Loss:\\n\\tlogLossTest = {0:.3f}'\n",
    "       .format(logLossTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
